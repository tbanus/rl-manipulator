# -*- coding: utf-8 -*-
"""rl_manipulator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tenqj-sWIKZq5rbFLv_KrKX5weZUxEmA
"""

# !pip install mujoco
# !pip install mujoco_mjx
# !pip install brax

#@title Import packages for plotting and creating graphics
import time
import itertools
import numpy as np
from typing import Callable, NamedTuple, Optional, Union, List

# Graphics and plotting.
print('Installing mediapy:')
# !command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)
# !pip install -q mediapy
import mediapy as media
import matplotlib.pyplot as plt

# More legible printing from numpy.
np.set_printoptions(precision=3, suppress=True, linewidth=100)

# Commented out IPython magic to ensure Python compatibility.
#@title Check if MuJoCo installation was successful

# from google.colab import files

import distutils.util
import os
import subprocess
if subprocess.run('nvidia-smi').returncode:
  raise RuntimeError(
      'Cannot communicate with GPU. '
      'Make sure you are using a GPU Colab runtime. '
      'Go to the Runtime menu and select Choose runtime type.')

# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.
# This is usually installed as part of an Nvidia driver package, but the Colab
# kernel doesn't install its driver via APT, and as a result the ICD is missing.
# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)
NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'
if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):
  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:
    f.write("""{
    "file_format_version" : "1.0.0",
    "ICD" : {
        "library_path" : "libEGL_nvidia.so.0"
    }
}
""")

# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs
xla_flags = os.environ.get('XLA_FLAGS', '')
xla_flags += ' --xla_gpu_triton_gemm_any=True'
os.environ['XLA_FLAGS'] = xla_flags

# Configure MuJoCo to use the EGL rendering backend (requires GPU)
print('Setting environment variable to use GPU rendering:')
# %env MUJOCO_GL=egl

try:
  print('Checking that the installation succeeded:')
  import mujoco
  mujoco.MjModel.from_xml_string('<mujoco/>')
except Exception as e:
  raise e from RuntimeError(
      'Something went wrong during installation. Check the shell output above '
      'for more information.\n'
      'If using a hosted Colab runtime, make sure you enable GPU acceleration '
      'by going to the Runtime menu and selecting "Choose runtime type".')

print('Installation successful.')

from datetime import datetime
import functools
from IPython.display import HTML
import jax
from jax import numpy as jp
import numpy as np
from typing import Any, Dict, Sequence, Tuple, Union

from brax import base
from brax import envs
from brax import math
from brax.base import Base, Motion, Transform
from brax.envs.base import Env, PipelineEnv, State
from brax.mjx.base import State as MjxState
from brax.training.agents.ppo import train as ppo
from brax.training.agents.ppo import networks as ppo_networks
from brax.io import html, mjcf, model

from etils import epath
from flax import struct
from matplotlib import pyplot as plt
import mediapy as media
from ml_collections import config_dict
import mujoco
from mujoco import mjx

mj_model = mujoco.MjModel.from_xml_path('unitree_z1/scene.xml')
mj_data = mujoco.MjData(mj_model)
renderer = mujoco.Renderer(mj_model)



#@title Sim Env

class Manipulator(PipelineEnv):

  def __init__(
      self,
      forward_reward_weight=1.25,
      ctrl_cost_weight=0.1,
      healthy_reward=5.0,
      terminate_when_unhealthy=True,
      healthy_z_range=(1.0, 2.0),
      reset_noise_scale=1e-2,
      exclude_current_positions_from_observation=True,
      **kwargs,
  ):

    mj_model = mujoco.MjModel.from_xml_path('unitree_z1/scene.xml')
    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG
    mj_model.opt.iterations = 6
    mj_model.opt.ls_iterations = 6

    sys = mjcf.load_model(mj_model)
    self.target_pos=[0.5, 0.0, 0.05]
    physics_steps_per_control_step = 5
    kwargs['n_frames'] = kwargs.get(
        'n_frames', physics_steps_per_control_step)
    kwargs['backend'] = 'mjx'

    super().__init__(sys, **kwargs)

    self._forward_reward_weight = forward_reward_weight
    self._ctrl_cost_weight = ctrl_cost_weight
    self._healthy_reward = healthy_reward
    self._terminate_when_unhealthy = terminate_when_unhealthy
    self._healthy_z_range = healthy_z_range
    self._reset_noise_scale = reset_noise_scale
    self._exclude_current_positions_from_observation = (
        exclude_current_positions_from_observation
    )

  def reset(self, rng: jp.ndarray) -> State:
    """Resets the environment to an initial state."""
    rng, rng1, rng2 = jax.random.split(rng, 3)

    low, hi = -self._reset_noise_scale, self._reset_noise_scale
    qpos = self.sys.qpos0 + jax.random.uniform(
        rng1, (self.sys.nq,), minval=low, maxval=hi
    )
    qvel = jax.random.uniform(
        rng2, (self.sys.nv,), minval=low, maxval=hi
    )

    data = self.pipeline_init(qpos, qvel)

    obs = self._get_obs(data, jp.zeros(self.sys.nu))
    reward, done, zero = jp.zeros(3)
    metrics = {
        'reward_hand_pos': zero,
        'cost_torque': zero,
        'cost_base_disturbance': zero,
        'j1_pos': zero,
        'j2_pos': zero,
        'j3_pos': zero,
        'j4_pos': zero,
        'j5_pos': zero,
        'j6_pos': zero,
        'load_pos_x': zero,
        'load_pos_y': zero,
        'load_pos_z': zero


    }
    return State(data, obs, reward, done, metrics)

  def step(self, state: State, action: jp.ndarray) -> State:
    """Run 1 timestep of the simulation."""
    data0 = state.pipeline_state
    print(state.pipeline_state)
    data = self.pipeline_step(data0, action)

    # com_before = data0.subtree_com[1]
    # com_after = data.subtree_com[1]
    # velocity = (com_after - com_before) / self.dt

    j1_pos=data.qpos[0]
    j2_pos=data.qpos[1]
    j3_pos=data.qpos[2]
    j4_pos=data.qpos[3]
    j5_pos=data.qpos[4]
    j6_pos=data.qpos[5]
    reward_hand_pos=0.0
    cost_torque=0.0
    cost_base_disturbance=0.0
    load_pos_x=data.qpos[6]
    load_pos_y=data.qpos[7]
    load_pos_z=data.qpos[8]
    # if self._terminate_when_unhealthy:
    #   healthy_reward = self._healthy_reward
    # else:
    #   healthy_reward = self._healthy_reward

    # ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))
    reward_hand_pos = self._forward_reward_weight *(  (load_pos_x-self.target_pos[0])**2+(load_pos_y-self.target_pos[1])**2+(load_pos_z-self.target_pos[2])**2)
    ctrl_cost =0.0
    for i in range(0,len(action)):
       ctrl_cost+=action[i]**2
    obs = self._get_obs(data, action)
    reward = reward_hand_pos  - ctrl_cost
    # load_position =  state.select.
    done = 1.0  if (  (load_pos_x-self.target_pos[0])**2+(load_pos_y-self.target_pos[1])**2+(load_pos_z-self.target_pos[2])**2)<0.01 else 0.0

    state.metrics.update(
        reward_hand_pos=reward_hand_pos,
        cost_torque=cost_torque,
        cost_base_disturbance=-cost_base_disturbance,
        j1_pos=j1_pos,
        j2_pos=j2_pos,
        j3_pos=j3_pos,
        j4_pos=j4_pos,
        j5_pos=j5_pos,
        j6_pos=j6_pos,
        load_pos_x=load_pos_x,
        load_pos_y=load_pos_y,
        load_pos_z=load_pos_z,
    )

    return state.replace(
        pipeline_state=data, obs=obs, reward=reward, done=done
    )

  def _get_obs(
      self, data: mjx.Data, action: jp.ndarray
  ) -> jp.ndarray:
    """Observes body position, velocities, and angles."""
    position = data.qpos
    # if self._exclude_current_positions_from_observation:
    #   position = position[2:]

    # external_contact_forces are excluded
    return jp.concatenate([
        position,
        data.qvel,
        data.cinert[1:].ravel(),
        data.cvel[1:].ravel(),
        data.qfrc_actuator,
    ])


envs.register_environment('manipulator', Manipulator)

"""## Visualize a Rollout

Let's instantiate the environment and visualize a short rollout.

NOTE: Since episodes terminates early if the torso is below the healthy z-range, the only relevant contacts for this task are between the feet and the plane. We turn off other contacts.
"""

# instantiate the environment
env_name = 'manipulator'
env = envs.get_environment(env_name)

# define the jit reset/step functions
jit_reset = jax.jit(env.reset)
jit_step = jax.jit(env.step)

# initialize the state
state = jit_reset(jax.random.PRNGKey(0))
rollout = [state.pipeline_state]

# grab a trajectory
for i in range(30):
  ctrl = -0.1 * jp.ones(env.sys.nu)
  state = jit_step(state, ctrl)
  rollout.append(state.pipeline_state)

media.show_video(env.render(rollout, camera='side'), fps=1.0 / env.dt)

"""## Train Policy

Let's now train a policy with PPO to make the sim run forwards.
"""

# train_fn = functools.partial(
#     ppo.train, num_timesteps=30_000, num_evals=5, reward_scaling=0.1,
#     episode_length=1000, normalize_observations=True, action_repeat=1,
#     unroll_length=10, num_minibatches=32, num_updates_per_batch=8,
#     discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048,
#     batch_size=1024, seed=0)


# x_data = []
# y_data = []
# ydataerr = []
# times = [datetime.now()]

# max_y, min_y = 13000, 0
# def progress(num_steps, metrics):
#   times.append(datetime.now())
#   x_data.append(num_steps)
#   y_data.append(metrics['eval/episode_reward'])
#   ydataerr.append(metrics['eval/episode_reward_std'])

#   plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])
#   plt.ylim([min_y, max_y])

#   plt.xlabel('# environment steps')
#   plt.ylabel('reward per episode')
#   plt.title(f'y={y_data[-1]:.3f}')

#   plt.errorbar(
#       x_data, y_data, yerr=ydataerr)
#   plt.show()

# make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)

# print(f'time to jit: {times[1] - times[0]}')
# print(f'time to train: {times[-1] - times[1]}')

# """<!-- ## Save and Load Policy -->

# We can save and load the policy using the brax model API.
# """

# #@title Save Model
# model_path = '/tmp/mjx_brax_policy'
# model.save_params(model_path, params)

# #@title Load Model and Define Inference Function
# params = model.load_params(model_path)

# inference_fn = make_inference_fn(params)
# jit_inference_fn = jax.jit(inference_fn)

# #@title Load Model and Define Inference Function
# params = model.load_params(model_path)

# inference_fn = make_inference_fn(params)
# jit_inference_fn = jax.jit(inference_fn)

# """## Visualize Policy

# Finally we can visualize the policy.
# """

# eval_env = envs.get_environment(env_name)

# jit_reset = jax.jit(eval_env.reset)
# jit_step = jax.jit(eval_env.step)

# # initialize the state
# rng = jax.random.PRNGKey(0)
# state = jit_reset(rng)
# rollout = [state.pipeline_state]

# # grab a trajectory
# n_steps = 500
# render_every = 2

# for i in range(n_steps):
#   act_rng, rng = jax.random.split(rng)
#   ctrl, _ = jit_inference_fn(state.obs, act_rng)
#   state = jit_step(state, ctrl)
#   rollout.append(state.pipeline_state)

#   if state.done:
#     break

# media.show_video(env.render(rollout[::render_every], camera='side'), fps=1.0 / env.dt / render_every)

# """# MJX Policy in MuJoCo

# We can also perform the physics step using the original MuJoCo python bindings to show that the policy trained in MJX works in MuJoCo.
# """